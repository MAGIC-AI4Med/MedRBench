# MedRBench 
The official code for "Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases".

## Introduction
In this paper, we assess the quality of reasoning processes in Large Language Models (LLMs) within real-world medical contexts. Our contributions are as follows:

1. We present the first quantitative analysis focused on **LLM reasoning process quality** in clinical scenarios—a domain inherently rich with ground-truth reasoning processes such as differential diagnosis. Our evaluation encompasses cutting-edge reasoning models, including Deepseek-R1, OpenAI's o3-mini, and others.
2. We have developed **MedRBench**, a benchmark for evaluating reasoning performance based on PMC-OA case reports. This benchmark is specifically designed to mirror the complexities of authentic clinical scenarios.
3. We introduce an innovative agentic system, the **Reasoning Evaluator**, engineered to automate and objectively quantify free-text reasoning responses in a scalable and efficient manner.

## Files Organsization
```
├── data/
│   └── MedRbench/            # Contains the benchmark data.
│   └── InferenceResults/     # Contains the model output in our experiments.
├── src/                      # Source code will be available soon.
├── README.md                 # General information and instructions.
```
## How to use
coming soon


